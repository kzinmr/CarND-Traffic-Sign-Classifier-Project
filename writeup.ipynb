{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Traffic Sign Recognition** \n",
    "\n",
    "---\n",
    "\n",
    "**Build a Traffic Sign Recognition Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Load the data set (see below for links to the project data set)\n",
    "* Explore, summarize and visualize the data set\n",
    "* Design, train and test a model architecture\n",
    "* Use the model to make predictions on new images\n",
    "* Analyze the softmax probabilities of the new images\n",
    "* Summarize the results with a written report\n",
    "\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1_train]: ./examples/train_hist.png \"Class distribution of training set\"\n",
    "[image1_validation]: ./examples/validation_hist.png \"Class distribution of validation set\"\n",
    "[image1_test]: ./examples/test_hist.png \"Class distribution of test set\"\n",
    "\n",
    "[image4]: ./examples/3.jpg \"Traffic Sign 1\"\n",
    "[image5]: ./examples/11.jpg \"Traffic Sign 2\"\n",
    "[image6]: ./examples/14.jpg \"Traffic Sign 3\"\n",
    "[image7]: ./examples/27.jpg \"Traffic Sign 4\"\n",
    "[image8]: ./examples/30.jpg \"Traffic Sign 5\"\n",
    "\n",
    "[image4c]: ./samples/3.jpg \"Traffic Sign 1(cropped)\"\n",
    "[image5c]: ./samples/11.jpg \"Traffic Sign 2(cropped)\"\n",
    "[image6c]: ./samples/14.jpg \"Traffic Sign 3(cropped)\"\n",
    "[image7c]: ./samples/27.jpg \"Traffic Sign 4(cropped)\"\n",
    "[image8c]: ./samples/30.jpg \"Traffic Sign 5(cropped)\"\n",
    "## Rubric Points\n",
    "###Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/481/view) individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---\n",
    "### Writeup / README\n",
    "\n",
    "Here is a link to my [project code](https://github.com/kzinmr/CarND-Traffic-Sign-Classifier-Project/blob/master/Traffic_Sign_Classifier.ipynb)\n",
    "\n",
    "\n",
    "### Data Set Summary & Exploration\n",
    "\n",
    "I used the numpy method to calculate summary statistics of the data set:\n",
    "\n",
    "* The size of training set is 31367\n",
    "* The size of the validation set is 3921\n",
    "* The size of test set is 3921\n",
    "* The shape of a traffic sign image is (32, 32, 3) which will be (36, 36, 3) after zero padding.\n",
    "* The number of unique classes/labels in the data set is 43\n",
    "\n",
    "####2. Include an exploratory visualization of the dataset.\n",
    "\n",
    "Here is an exploratory visualization of the data set. It is a bar chart showing\n",
    "the distribution of each class labels in training/validation/test set respectively.\n",
    "\n",
    "![alt text][image1_train]\n",
    "![alt text][image1_validation]\n",
    "![alt text][image1_test]\n",
    "\n",
    "\n",
    "### Design and Test a Model Architecture\n",
    "\n",
    "#### Description of my preprocessing\n",
    "I didn't use grayscale conversion because I got better results without this.\n",
    "Zero padding is added because I use valid padding in my CNN layers.\n",
    "As a last step, I normalized the image data into the range [-1,1].\n",
    "\n",
    "#### Description of my model architecture\n",
    "My model consists of two convolution layers and average poolings, and two fully connected layers.\n",
    "Weight parameter initialization is done with Xavier initialization.\n",
    "Batch normalization technique especially offers my model good performance.\n",
    "\n",
    "Layers of My final model are summarized as following:\n",
    "\n",
    "| Layer         \t\t|     Description\t        \t\t\t\t\t| \n",
    "|:---------------------:|:---------------------------------------------:| \n",
    "| Input         \t\t| 36x36x3 RGB image   \t\t\t\t\t\t\t| \n",
    "| Convolution 5x5     \t| 1x1 stride, valid padding, outputs 32x32x16 \t|\n",
    "| Batch Normalization\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Dropout\t\t\t\t| drop probability is 0.5\t\t\t\t\t\t|\n",
    "| Average pooling 2x2\t| 1x1 stride, valid padding,  outputs 32x32x16 \t|\n",
    "| Convolution 5x5     \t| 1x1 stride, valid padding, outputs 30x30x32 \t|\n",
    "| Batch Normalization\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Dropout\t\t\t\t| drop probability is 0.5\t\t\t\t\t\t|\n",
    "| Average pooling 2x2\t| 2x2 stride, valid padding,  outputs 15x15x32 \t|\n",
    "| Fully connected\t\t| outputs 256      \t\t\t\t\t\t\t\t|\n",
    "| Batch Normalization\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Dropout\t\t\t\t| drop probability is 0.5\t\t\t\t\t\t|\n",
    "| Fully connected\t\t| outputs 128      \t\t\t\t\t\t\t\t|\n",
    "| Batch Normalization\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Dropout\t\t\t\t| drop probability is 0.5\t\t\t\t\t\t|\n",
    "| Fully connected\t\t| outputs 43      \t\t\t\t\t\t\t\t|\n",
    "| Softmax\t\t\t\t| etc.        \t\t\t\t\t\t\t\t\t|\n",
    "\n",
    "\n",
    "#### Description of my hyper parameters selection\n",
    "I used the Adam optimizer because it provides decent performance.\n",
    "Learning rate and batch size were chosen from a few experiments. Keep probability was fixed to 0.5. \n",
    "For the number of epochs 10, accuracy on the validation set seems high enough (99.5%).\n",
    "\n",
    "#### Discussion of my final model\n",
    "\n",
    "\n",
    "My final model results were:\n",
    "* training set accuracy of  99.904 %\n",
    "* validation set accuracy of 99.2 %\n",
    "* test set accuracy of 99.2 %\n",
    "\n",
    "The main factor of the validation set accuracy below 0.93 was the batch normalization technique.\n",
    "\n",
    "If an iterative approach was chosen:\n",
    "* I first choose the single convolution layer and single fully connected layer architecture with dropout which resulted in poor performance around 80% validation set accuracy (underfitting).\n",
    "* Next I tried the same architecture as the final one without batch normalization which gave around 85% accuracy (maybe overfitting).\n",
    "* Finally I achieved the final performance by adding batch normalization.\n",
    "* I adjusted the volume sizes of each layers with respect to the number of classes by comparing LeNet weight sizes.\n",
    "* After I found out the final architecture, I tuned the batch size and number of epochs in order to achieve decent accuracy.\n",
    "* LeNet architecture helps with this problem because convolutional layer could capture the local features of each classes of traffic signs.\n",
    "* I couldn't find out why batch normalization works so well compared with dropout.\n",
    " \n",
    "\n",
    "### Test a Model on New Images\n",
    "\n",
    "#### Description of my test images\n",
    "I collect original five images from the web.\n",
    "\n",
    "Here are five German traffic signs that I found on the web:\n",
    "\n",
    "![alt text][image4] ![alt text][image5] ![alt text][image6] \n",
    "![alt text][image7] ![alt text][image8]\n",
    "\n",
    "All of these skew images except Image 3 were unable to classify at top-1 by my model(ACC 20%).\n",
    "So I cropped these images into square manually. Then the accuracy was improved to 80 %.\n",
    "Here are the cropped images actually used in my prediction:\n",
    "![alt text][image4c] ![alt text][image5c] ![alt text][image6c] \n",
    "![alt text][image7c] ![alt text][image8c]\n",
    "\n",
    "#### Description of my final predictions\n",
    "\n",
    "Here are the results of the prediction:\n",
    "\n",
    "| Image\t\t\t        |     Prediction\t        \t\t\t\t\t| \n",
    "|:---------------------:|:---------------------------------------------:| \n",
    "| 60km/h        \t\t| 60km/h    \t\t\t\t\t\t\t\t\t| \n",
    "| Right-of-way \t\t\t| Right-of-way \t\t\t\t\t\t\t\t\t|\n",
    "| Stop\t\t\t\t\t| Stop\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Pedestrians\t      \t| Pedestrians\t\t\t\t\t \t\t\t\t|\n",
    "| Beware of ice/snow\t| 100km/h           \t\t\t\t\t\t\t|\n",
    "\n",
    "\n",
    "The model was able to correctly guess 4 of the 5 traffic signs, which gives an accuracy of 80%. \n",
    "The image of 'Beware of ice/snow' (30.jpg) was still unrecognized. \n",
    "From the observation of feature maps, I guess it is because the resolution is too small to grasp the detail of the sign.\n",
    "\n",
    "#### Description of the certainty\n",
    "\n",
    "The top five soft max probabilities were\n",
    "\n",
    "| Probability         \t|     Prediction\t        \t\t\t\t\t| \n",
    "|:---------------------:|:---------------------------------------------:| \n",
    "| .99         \t\t\t| 60km/h    \t\t\t\t\t\t\t\t\t| \n",
    "| .99     \t\t\t\t| Right-of-way\t\t\t\t\t\t\t\t\t|\n",
    "| .99\t\t\t\t\t| Stop\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| .98\t      \t\t\t| Pedestrians\t\t\t\t\t \t\t\t\t|\n",
    "| .99\t\t\t\t    | 100km/h           \t\t\t\t\t\t\t|\n",
    "\n",
    "\n",
    "From these results, my model perform well on the cropped data although it seems ovefitting for each classes.\n",
    "\n",
    "### (Optional) Visualizing the Neural Network (See Step 4 of the Ipython notebook for more details)\n",
    "\n",
    "From the obtained feature maps of two convolution layers, we could interprete the first layer feature maps, but couldn't the second layer ones obviously. For '30.jpg(Beware of ice/snow)', it's difficult to interprete even the first feature map."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
